\svnid{$Id: testbench.tex 50935 2017-05-12 21:09:46Z mooiman $}

\chapter{Test bench}

\section{Introduction}

The test bench of DFLOW-FM is an important tool to keep all parts working as aspected,
while developing new features, fixing bugs or reorganizing the code.

The first section describes where to find the scripts of the test bench,
the test cases, the location of the website to monitor the results.
The second section describes how to add a new test case.

The test bench runs automatically: if somethings changes in the source code of DFLOW-FM,
or in the test scripts or test cases, new executables are build, and tests are run,
and the tests results are compared with reference results.

The test scripts are also used for other projects than \DFLOWFM (e.g.\ \DFLOW and \DWAQ),
and works on Windows and Linux.

\section{Location of all parts of the test bench}

TeamCity (see \url{https://www.jetbrains.com/teamcity}) is used as the test bench environment.
The overall project page for DFlowFM with TeamCity is located at
\url{https://build.deltares.nl/project.html?projectId=DFlowFlexibleMesh&tab=projectOverview} .
On this page you will find three subprojects: 3Di-related, Build FM distributions and Testbenches FM.

The svn repository of the test bench scripts is: \\
\url{https://repos.deltares.nl/repos/DSCTestbench/scripts/trunk}.

The test bench itself is a python project with as the main file: {\tt TestBench.py}.
As a default within Deltares, we use Anaconda (\url{https://anaconda.org/}) as the python environment.

The test cases itself are located on two locations:
the input of the cases are at:
\url{https://repos.deltares.nl/repos/DSCTestbench/cases/trunk/e02_dflowfm},
the output to which we compare our results (references) are at:
\url{https://repos.deltares.nl/repos/DSCTestbench/references/trunk}.


\section{How to add a new test case}

We assume that the test case input is already in the right location in the repository,
if not, add the test case to it.

To add test cases, you should first choose the configuration where your test case belongs to.
There are currently about 60 configurations. The most important configurations for DFLOW-FM are:
\begin{itemize}
 \item dflowfm\_win64.xml
 \item dflowfm\_lnx64\_single.xml
 \item dflowfm\_lnx64\_parallel.xml
 \item dflowfm\_3dcases\_win64.xml
\end{itemize}

As the extension suggests, the configuration is an xml file.
The xml file is defined by its xsd:
\url{https://repos.deltares.nl/repos/DSCTestbench/scripts/configs/deltaresTestbench_v3-2.00.xsd}

\newpage

Each test case is defined within the tags <testCase>.
An example is:
\begin{Verbatim}
<testCase name="e02_f14_c060_sill_free_griddir" ref="dflowfm_default">
   <path>e02_dflowfm/f14_parallel/c060_sill_free_griddirection</path>
   <programs>
     <program ref="DFlowFM">
         <arguments>
            <argument>weirfree.mdu</argument>
            <argument>--autostartstop</argument>
            <argument>--nodisplay</argument>
         </arguments>
     </program>
   </programs>
   <maxRunTime>15000.0</maxRunTime>
   <checks>
     <file name="dflowfmoutput/pillar_small_0000_his.nc" type="netCDF">
       <parameters>
         <parameter name="waterlevel" toleranceAbsolute="0.00001" />
         <parameter name="x_velocity" toleranceAbsolute="0.00001" />
         <parameter name="y_velocity" toleranceAbsolute="0.00001" />
       </parameters>
     </file>
   </checks>
</testCase>  
\end{Verbatim}

Most settings can be copied from this example.
Next settings must be altered:

\begin{tabular}{|l|l|}
 \hline
 \textbf{tag} & \textbf{description} \\
 \hline
 name      & identification for test case \\
 path      & relative path to input of test case \\
 argument  & name of the mdu-file \\
 file name & path to output path \\
 type      & type of output file \\
 parameter & variable to compare with selected tolerance (absolute or relative) \\
 \hline
\end{tabular}

Once the test case is included in the configuration file,
the test script can be used to run this case to produce references.

\begin{Verbatim}
python --config dflowfm_lnx64_parallel.xml --filter f14_c060 -r
\end{Verbatim}

The option -r is for reference run.

If everything goes right, the reference value can be found in the directory: \\
data/references/trunk/<platform>/<testset1>/<testset2>/<name> .

If everything looks fine
(which can be checked, locally, to run the testscript with the -c option, for comparison),
these references can be submitted to the repository.

Finally check in the changes to the configuration file.

Repeat the steps for the other configurations.
